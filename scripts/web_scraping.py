from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
import time
from utils.gerar_aquivo import salvar_em_excel


class WebScraperComPaginacao:
    def __init__(self, driver_path=None):
        """
        Inicializa o WebDriver do Selenium.

        Args:
            driver_path: Caminho para o chromedriver (opcional)
        """
        if driver_path:
            service = Service(driver_path)
            self.driver = webdriver.Chrome(service=service)
        else:
            self.driver = webdriver.Chrome()

        self.wait = WebDriverWait(self.driver, 10)

    def acessar_pagina(self, url):
        """
        Acessa uma URL e aguarda carregamento.

        Args:
            url: URL a ser acessada
        """
        self.driver.get(url)
        time.sleep(2)

    def extrair_linhas_da_pagina(self, section_selector, li_selector):
        """
        Extrai todas as linhas (li) dentro da section na p√°gina atual.
        Exclui elementos de pagina√ß√£o.

        Args:
            section_selector: Seletor CSS ou XPath da section
            li_selector: Seletor CSS ou XPath dos elementos li

        Returns:
            Lista com os hrefs encontrados na p√°gina
        """
        try:
            # Aguarda a section estar presente
            section = self.wait.until(
                EC.presence_of_element_located(
                    (By.CSS_SELECTOR, section_selector))
            )

            # Extrai todos os links (li) dentro da section
            linhas = section.find_elements(By.CSS_SELECTOR, li_selector)

            print(
                f"Elementos li encontrados (antes da filtragem): {len(linhas)}")

            hrefs = []
            for linha in linhas:
                try:
                    # Verifica se √© um elemento de pagina√ß√£o
                    classes = linha.get_attribute("class")

                    # Pula se for parte do pager
                    if "pager" in classes or "next" in classes or "current" in classes or "prev" in classes:
                        print(f"  ‚äò Ignorado: elemento de pagina√ß√£o")
                        continue

                    link = linha.find_element(By.TAG_NAME, "a")
                    href = link.get_attribute("href")

                    if href:
                        hrefs.append(href)

                except Exception as e:
                    continue

            print(
                f"‚úì Total de produtos extra√≠dos (ap√≥s filtragem): {len(hrefs)}\n")
            return hrefs

        except Exception as e:
            print(f"Erro ao extrair linhas: {e}")
            return []

    # Extra√ß√£o dos dados da p√°gina de produto
    def extrair_informacoes(self, url):
        """
        Acessa uma URL e extrai as informa√ß√µes desejadas.

        Args:
            url: URL da p√°gina a ser analisada

        Returns:
            Dicion√°rio com as informa√ß√µes extra√≠das
        """
        try:
            self.driver.get(url)
            time.sleep(1)

            informacoes = {
                'url': url,
                'titulo': '',
                'descricao': '',
                'preco': '',
                'rating': '',
                'disponibilidade': '',
                'categoria': '',
                'imagem_url': ''
            }

            # Extrai o t√≠tulo
            try:
                titulo = self.driver.find_element(By.TAG_NAME, "h1")
                informacoes['titulo'] = titulo.text
            except:
                informacoes['titulo'] = 'T√≠tulo n√£o encontrado'

            # Extrai descri√ß√£o
            try:
                secao_descricao = self.driver.find_element(
                    By.CSS_SELECTOR, 'article.product_page')
                paragrafos = secao_descricao.find_elements(By.TAG_NAME, "p")
                informacoes['descricao'] = paragrafos[3].text
            except:
                informacoes['descricao'] = 'Descri√ß√£o n√£o encontrado'

            # Extrai pre√ßo
            try:
                preco = self.driver.find_element(By.CLASS_NAME, "price_color")
                informacoes['preco'] = preco.text.replace('¬£', '')
            except:
                informacoes['preco'] = 'Pre√ßo n√£o encontrado'

            # Rating
            try:
                informacoes['rating'] = self.extrair_rating(
                    url)
            except:
                informacoes['rating'] = 'Rating n√£o encontrado'

            # Disponibilidade
            try:
                # Seleciona a classe <p instock availability>
                stock = self.driver.find_element(
                    By.CSS_SELECTOR, 'p.instock.availability')
                # XPath para o i dentro dela
                disponibilidade = stock.find_element(By.XPATH, "i")

                if disponibilidade.get_attribute('class'):
                    informacoes['disponibilidade'] = 1
                else:
                    informacoes['disponibilidade'] = 0
            except:
                informacoes['disponibilidade'] = 'Disponibilidade n√£o encontrada'

            # Categoria
            try:
                # Encontra a categoria do produto
                breadcrumb = self.driver.find_element(
                    By.CSS_SELECTOR, 'div.page_inner ul.breadcrumb')
                # XPath para o terceiro li
                categoria = breadcrumb.find_element(By.XPATH, "li[3]")

                informacoes["categoria"] = categoria.text
            except:
                informacoes["categoria"] = 'Categoria n√£o encontrada'

            # URL da imagem
            try:
                # Seleciona a classe <div class="item active">
                item_active = self.driver.find_element(
                    By.CSS_SELECTOR, 'div.item.active')
                # XPath para o img dentro dela
                imagem = item_active.find_element(By.XPATH, "img")

                informacoes['imagem_url'] = imagem.get_attribute('src')
            except:
                informacoes['imagem_url'] = 'Imagem n√£o encontrada'

            return informacoes

        except Exception as e:
            print(f"Erro ao extrair informa√ß√µes de {url}: {e}")
            return None

    # Fun√ß√£o auxiliar para extra√ß√£o da avalia√ß√£o por estrelas
    def extrair_rating(self, url_detalhes):
        """
        Extrai o rating de estrelas e converte em n√∫mero.
        O rating √© representado por por uma classe CSS com dois nomes:
        "star-rating" e o nome do rating em ingl√™s (ex: "Three").

        Args:
            url_detalhes: URL da p√°gina do produto

        Returns:
            Numero de estrelas (0-5)
        """

        # Dicion√°rio de convers√£o
        conversao = {
            'Zero': 0,
            'One': 1,
            'Two': 2,
            'Three': 3,
            'Four': 4,
            'Five': 5
        }

        try:
            rating_element = self.driver.find_element(
                By.CSS_SELECTOR, "p.star-rating")
            classes = rating_element.get_attribute("class")

            # Pega todas as classes
            classes_lista = classes.split()

            # A segunda classe √© o rating em ingl√™s
            rating_texto = classes_lista[1]

            # Converte para n√∫mero
            rating_numero = conversao.get(rating_texto, 0)

            # print(f"‚úì Rating: {rating_numero} estrelas")
            return rating_numero

        except Exception as e:
            print(f"‚úó Erro ao extrair rating: {e}")
        return None

    # Verifica pr√≥xima p√°gina
    def verificar_proxima_pagina(self, next_page_selector):
        """
        Verifica se existe bot√£o/link para pr√≥xima p√°gina.

        Args:
            next_page_selector: Seletor para o link da pr√≥xima p√°gina

        Returns:
            URL da pr√≥xima p√°gina ou None se n√£o existir
        """
        try:
            # Tenta encontrar o elemento de pr√≥xima p√°gina
            next_button = self.driver.find_element(
                By.CSS_SELECTOR,
                next_page_selector
            )

            # Extrai a URL da pr√≥xima p√°gina
            href = next_button.get_attribute("href")

            if href:
                # Converte URL relativa para absoluta se necess√°rio
                if not href.startswith("http"):
                    base_url = self.driver.current_url.split("/catalogue/")[0]
                    href = base_url + "/catalogue/" + \
                        href.replace("catalogue/", "")

                print(f"‚úì Pr√≥xima p√°gina encontrada: {href}")
                return href

            print("‚úó Bot√£o pr√≥xima p√°gina n√£o tem href v√°lido")
            return None

        except:
            print("‚úó N√£o h√° pr√≥xima p√°gina (elemento n√£o encontrado)")
            return None

    # Obt√©m p√°gina atual
    def obter_pagina_atual(self):
        """
        Extrai o n√∫mero da p√°gina atual da estrutura de pagina√ß√£o.

        Returns:
            String com informa√ß√£o da p√°gina (ex: "Page 1 of 50")
        """
        try:
            pager_current = self.driver.find_element(
                By.CSS_SELECTOR,
                "ul.pager li.current"
            )
            return pager_current.text
        except:
            return "P√°gina desconhecida"

    # Fun√ß√£o pricipal para processar todas as p√°ginas
    def processar_todas_paginas(
        self,
        url_inicial,
        section_selector,
        li_selector,
        next_page_selector,
        max_paginas=None
    ):
        """
        Processa todas as p√°ginas: extrai 20 produtos por p√°gina e navega.

        Args:
            url_inicial: URL da primeira p√°gina
            section_selector: Seletor da section
            li_selector: Seletor dos elementos li
            next_page_selector: Seletor do bot√£o/link pr√≥xima p√°gina
            max_paginas: Limite de p√°ginas a processar (None = todas)

        Returns:
            Lista com todas as informa√ß√µes coletadas
        """
        url_atual = url_inicial
        pagina_numero = 1
        dados_coletados = []
        produtos_total = 0

        while url_atual and (max_paginas is None or pagina_numero <= max_paginas):
            print(f"\n{'='*70}")
            print(f"PROCESSANDO P√ÅGINA {pagina_numero}")
            print(f"{'='*70}")
            print(f"URL: {url_atual}")

            # Acessa a p√°gina
            self.acessar_pagina(url_atual)

            # Exibe informa√ß√£o de pagina√ß√£o
            info_paginacao = self.obter_pagina_atual()
            print(f"Status: {info_paginacao}\n")

            # Extrai os hrefs da p√°gina atual excluindo a pagina√ß√£o
            hrefs = self.extrair_linhas_da_pagina(
                section_selector, li_selector)

            if not hrefs:
                print("‚ö† Nenhum link encontrado nesta p√°gina. Encerrando.")
                break

            # Processa cada URL da p√°gina
            for indice, href in enumerate(hrefs, 1):
                produtos_total += 1
                print(
                    f"[P√°g {pagina_numero}] Produto {indice}/{len(hrefs)} (Total: {produtos_total})")
                print(f"URL: {href}")

                informacoes = self.extrair_informacoes(href)

                if informacoes:
                    dados_coletados.append(informacoes)
                    titulo_curto = informacoes['titulo'][:
                                                         50] if informacoes['titulo'] else 'Sem t√≠tulo'
                    print(f"‚úì Sucesso | T√≠tulo: {titulo_curto}")
                    if informacoes['preco']:
                        print(f"  Pre√ßo: {informacoes['preco']}")
                    if informacoes['descricao']:
                        print(f"  Descri√ß√£o: {informacoes['descricao']})")
                else:
                    print(f"‚úó Erro ao processar produto")

                print()  # Linha em branco para legibilidade

                # Pausa entre requisi√ß√µes
                time.sleep(0.5)

            # Retorna √† p√°gina de listagem para navegar
            print(f"Retornando √† p√°gina de listagem para pr√≥xima navega√ß√£o...")
            self.acessar_pagina(url_atual)
            time.sleep(1)

            # Procura pr√≥xima p√°gina
            url_atual = self.verificar_proxima_pagina(next_page_selector)
            pagina_numero += 1

        return dados_coletados

    def fechar(self):
        """Fecha o navegador."""
        self.driver.quit()


# === USADO NOS TESTES DE CONTRU√á√ÉO ===
if __name__ == "__main__":
    # Configura√ß√£o
    URL_INICIAL = "https://books.toscrape.com/index.html"
    SECTION_SELECTOR = "section"  # Seletor da se√ß√£o onde st√£o a linhas dos produtos

    # Selertor LI para capturar os produtos
    LI_SELECTOR = "li.col-xs-6.col-sm-4.col-md-3.col-lg-3"

    # Seletor de pr√≥xima p√°gina
    NEXT_PAGE_SELECTOR = "ul.pager li.next a"

    # Criar inst√¢ncia do scraper
    scraper = WebScraperComPaginacao()

    try:
        # Processa todas as p√°ginas
        resultados = scraper.processar_todas_paginas(
            URL_INICIAL,
            SECTION_SELECTOR,
            LI_SELECTOR,
            NEXT_PAGE_SELECTOR,
            max_paginas=None  # None = todas
        )

        # Exibe resumo dos resultados
        print(f"\n{'='*70}")
        print(f"üéâ RESUMO FINAL")
        print(f"{'='*70}")
        print(f"Total de produtos processados: {len(resultados)}")
        print(f"{'='*70}\n")

        # # Exibe os primeiros 5 produtos
        # for idx, resultado in enumerate(resultados, 1):
        #     print(f"{idx}. {resultado['titulo']}")
        #     print(f"   URL: {resultado['url']}")
        #     if resultado['preco']:
        #         print(f"   Pre√ßo: {resultado['preco']}\n")
        #         if resultado['descricao']:
        #             print(f"   Descri√ß√£o: {resultado['descricao']}\n")

        if resultados:
            print("\n" + "="*70)
            print("üíæ SALVANDO DADOS EM ARQUIVO...")
            print("="*70)

            # Grava os dados em Excel
            salvar_em_excel(resultados, caminho_pasta="dados",
                            nome_arquivo="catalogo_de_livros.xlsx", auto_versionar=False)

        print("\n‚úì Processo conclu√≠do com sucesso.")

    except KeyboardInterrupt:
        print("\n\n‚ö† Execu√ß√£o interrompida pelo usu√°rio.")

    except Exception as e:
        print(f"\nErro durante execu√ß√£o: {e}")

    finally:
        scraper.fechar()
