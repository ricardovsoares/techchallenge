from fastapi import FastAPI, BackgroundTasks, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Optional, Dict
import uvicorn
import asyncio
from concurrent.futures import ThreadPoolExecutor
import uuid
import threading
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
import time
from utils.gerar_aquivo import salvar_em_excel, salvar_em_csv
from utils.configs import settings
from models.scraping_model import ConfiguracaoScraper, RespostaExecucao

from utils.estado import atualizar_tarefa, obter_tarefa


class WebScraperComPaginacao:
    def __init__(self, driver_path=None):
        """
        Inicializa o WebDriver do Selenium.

        Args:
            driver_path: Caminho para o chromedriver (opcional)
        """
        if driver_path:
            service = Service(driver_path)
            self.driver = webdriver.Chrome(service=service)
        else:
            self.driver = webdriver.Chrome()

        self.wait = WebDriverWait(self.driver, 10)

    def acessar_pagina(self, url):
        """
        Acessa uma URL e aguarda carregamento.

        Args:
            url: URL a ser acessada
        """
        self.driver.get(url)
        time.sleep(2)

    def extrair_linhas_da_pagina(self, section_selector, li_selector):
        """
        Extrai todas as linhas (li) dentro da section na p√°gina atual.
        Exclui elementos de pagina√ß√£o.

        Args:
            section_selector: Seletor CSS ou XPath da section
            li_selector: Seletor CSS ou XPath dos elementos li

        Returns:
            Lista com os hrefs encontrados na p√°gina
        """
        try:
            # Aguarda a section estar presente
            section = self.wait.until(
                EC.presence_of_element_located(
                    (By.CSS_SELECTOR, section_selector))
            )

            # Extrai todos os links (li) dentro da section
            linhas = section.find_elements(By.CSS_SELECTOR, li_selector)

            print(
                f"Elementos li encontrados (antes da filtragem): {len(linhas)}")

            hrefs = []
            for linha in linhas:
                try:
                    # Verifica se √© um elemento de pagina√ß√£o
                    classes = linha.get_attribute("class")

                    # Pula se for parte do pager
                    if "pager" in classes or "next" in classes or "current" in classes or "prev" in classes:
                        print(f"  ‚äò Ignorado: elemento de pagina√ß√£o")
                        continue

                    link = linha.find_element(By.TAG_NAME, "a")
                    href = link.get_attribute("href")

                    if href:
                        hrefs.append(href)

                except Exception as e:
                    continue

            print(
                f"‚úì Total de produtos extra√≠dos (ap√≥s filtragem): {len(hrefs)}\n")
            return hrefs

        except Exception as e:
            print(f"Erro ao extrair linhas: {e}")
            return []

    # Extra√ß√£o dos dados da p√°gina de produto
    def extrair_informacoes(self, url):
        """
        Acessa uma URL e extrai as informa√ß√µes desejadas.

        Args:
            url: URL da p√°gina a ser analisada

        Returns:
            Dicion√°rio com as informa√ß√µes extra√≠das
        """
        try:
            self.driver.get(url)
            time.sleep(1)

            informacoes = {
                'url': url,
                'titulo': '',
                'descricao': '',
                'preco': '',
                'rating': '',
                'disponibilidade': '',
                'categoria': '',
                'imagem_url': ''
            }

            # Extrai o t√≠tulo
            try:
                titulo = self.driver.find_element(By.TAG_NAME, "h1")
                informacoes['titulo'] = titulo.text
            except:
                informacoes['titulo'] = 'T√≠tulo n√£o encontrado'

            # Extrai descri√ß√£o
            try:
                secao_descricao = self.driver.find_element(
                    By.CSS_SELECTOR, 'article.product_page')
                paragrafos = secao_descricao.find_elements(By.TAG_NAME, "p")
                informacoes['descricao'] = paragrafos[3].text
            except:
                informacoes['descricao'] = 'Descri√ß√£o n√£o encontrado'

            # Extrai pre√ßo
            try:
                preco = self.driver.find_element(By.CLASS_NAME, "price_color")
                informacoes['preco'] = preco.text.replace('¬£', '')
            except:
                informacoes['preco'] = 'Pre√ßo n√£o encontrado'

            # Rating
            try:
                informacoes['rating'] = self.extrair_rating(
                    url)
            except:
                informacoes['rating'] = 'Rating n√£o encontrado'

            # Disponibilidade
            try:
                # Seleciona a classe <p instock availability>
                stock = self.driver.find_element(
                    By.CSS_SELECTOR, 'p.instock.availability')
                # XPath para o i dentro dela
                disponibilidade = stock.find_element(By.XPATH, "i")

                if disponibilidade.get_attribute('class'):
                    informacoes['disponibilidade'] = 1
                else:
                    informacoes['disponibilidade'] = 0
            except:
                informacoes['disponibilidade'] = 'Disponibilidade n√£o encontrada'

            # Categoria
            try:
                # Encontra a categoria do produto
                breadcrumb = self.driver.find_element(
                    By.CSS_SELECTOR, 'div.page_inner ul.breadcrumb')
                # XPath para o terceiro li
                categoria = breadcrumb.find_element(By.XPATH, "li[3]")

                informacoes["categoria"] = categoria.text
            except:
                informacoes["categoria"] = 'Categoria n√£o encontrada'

            # URL da imagem
            try:
                # Seleciona a classe <div class="item active">
                item_active = self.driver.find_element(
                    By.CSS_SELECTOR, 'div.item.active')
                # XPath para o img dentro dela
                imagem = item_active.find_element(By.XPATH, "img")

                informacoes['imagem_url'] = imagem.get_attribute('src')
            except:
                informacoes['imagem_url'] = 'Imagem n√£o encontrada'

            return informacoes

        except Exception as e:
            print(f"Erro ao extrair informa√ß√µes de {url}: {e}")
            return None

    # Fun√ß√£o auxiliar para extra√ß√£o da avalia√ß√£o por estrelas
    def extrair_rating(self, url_detalhes):
        """
        Extrai o rating de estrelas e converte em n√∫mero.
        O rating √© representado por por uma classe CSS com dois nomes:
        "star-rating" e o nome do rating em ingl√™s (ex: "Three").

        Args:
            url_detalhes: URL da p√°gina do produto

        Returns:
            Numero de estrelas (0-5)
        """

        # Dicion√°rio de convers√£o
        conversao = {
            'Zero': 0,
            'One': 1,
            'Two': 2,
            'Three': 3,
            'Four': 4,
            'Five': 5
        }

        try:
            rating_element = self.driver.find_element(
                By.CSS_SELECTOR, "p.star-rating")
            classes = rating_element.get_attribute("class")

            # Pega todas as classes
            classes_lista = classes.split()

            # A segunda classe √© o rating em ingl√™s
            rating_texto = classes_lista[1]

            # Converte para n√∫mero
            rating_numero = conversao.get(rating_texto, 0)

            # print(f"‚úì Rating: {rating_numero} estrelas")
            return rating_numero

        except Exception as e:
            print(f"‚úó Erro ao extrair rating: {e}")
        return None

    # Verifica pr√≥xima p√°gina
    def verificar_proxima_pagina(self, next_page_selector):
        """
        Verifica se existe bot√£o/link para pr√≥xima p√°gina.

        Args:
            next_page_selector: Seletor para o link da pr√≥xima p√°gina

        Returns:
            URL da pr√≥xima p√°gina ou None se n√£o existir
        """
        try:
            # Tenta encontrar o elemento de pr√≥xima p√°gina
            next_button = self.driver.find_element(
                By.CSS_SELECTOR,
                next_page_selector
            )

            # Extrai a URL da pr√≥xima p√°gina
            href = next_button.get_attribute("href")

            if href:
                # Converte URL relativa para absoluta se necess√°rio
                if not href.startswith("http"):
                    base_url = self.driver.current_url.split("/catalogue/")[0]
                    href = base_url + "/catalogue/" + \
                        href.replace("catalogue/", "")

                print(f"‚úì Pr√≥xima p√°gina encontrada: {href}")
                return href

            print("‚úó Bot√£o pr√≥xima p√°gina n√£o tem href v√°lido")
            return None

        except:
            print("‚úó N√£o h√° pr√≥xima p√°gina (elemento n√£o encontrado)")
            return None

    # Obt√©m p√°gina atual
    def obter_pagina_atual(self):
        """
        Extrai o n√∫mero da p√°gina atual da estrutura de pagina√ß√£o.

        Returns:
            String com informa√ß√£o da p√°gina (ex: "Page 1 of 50")
        """
        try:
            pager_current = self.driver.find_element(
                By.CSS_SELECTOR,
                "ul.pager li.current"
            )
            return pager_current.text
        except:
            return "P√°gina desconhecida"

    # Fun√ß√£o pricipal para processar todas as p√°ginas
    def processar_todas_paginas(
        self,
        url_inicial,
        section_selector,
        li_selector,
        next_page_selector,
        max_paginas=None
    ):
        """
        Processa todas as p√°ginas: extrai 20 produtos por p√°gina e navega.

        Args:
            url_inicial: URL da primeira p√°gina
            section_selector: Seletor da section
            li_selector: Seletor dos elementos li
            next_page_selector: Seletor do bot√£o/link pr√≥xima p√°gina
            max_paginas: Limite de p√°ginas a processar (None = todas)

        Returns:
            Lista com todas as informa√ß√µes coletadas
        """
        url_atual = url_inicial
        pagina_numero = 1
        dados_coletados = []
        produtos_total = 0

        while url_atual and (max_paginas is None or pagina_numero <= max_paginas):
            print(f"\n{'='*70}")
            print(f"PROCESSANDO P√ÅGINA {pagina_numero}")
            print(f"{'='*70}")
            print(f"URL: {url_atual}")

            # Acessa a p√°gina
            self.acessar_pagina(url_atual)

            # Exibe informa√ß√£o de pagina√ß√£o
            info_paginacao = self.obter_pagina_atual()
            print(f"Status: {info_paginacao}\n")

            # Extrai os hrefs da p√°gina atual excluindo a pagina√ß√£o
            hrefs = self.extrair_linhas_da_pagina(
                section_selector, li_selector)

            if not hrefs:
                print("‚ö† Nenhum link encontrado nesta p√°gina. Encerrando.")
                break

            # Processa cada URL da p√°gina
            for indice, href in enumerate(hrefs, 1):
                produtos_total += 1
                print(
                    f"[P√°g {pagina_numero}] Produto {indice}/{len(hrefs)} (Total: {produtos_total})")
                print(f"URL: {href}")

                informacoes = self.extrair_informacoes(href)

                if informacoes:
                    dados_coletados.append(informacoes)
                    titulo_curto = informacoes['titulo'][:
                                                         50] if informacoes['titulo'] else 'Sem t√≠tulo'
                    print(f"‚úì Sucesso | T√≠tulo: {titulo_curto}")
                    if informacoes['preco']:
                        print(f"  Pre√ßo: {informacoes['preco']}")
                    if informacoes['descricao']:
                        print(f"  Descri√ß√£o: {informacoes['descricao']})")
                else:
                    print(f"‚úó Erro ao processar produto")

                print()  # Linha em branco para legibilidade

                # Pausa entre requisi√ß√µes
                time.sleep(0.5)

            # Retorna √† p√°gina de listagem para navegar
            print(f"Retornando √† p√°gina de listagem para pr√≥xima navega√ß√£o...")
            self.acessar_pagina(url_atual)
            time.sleep(1)

            # Procura pr√≥xima p√°gina
            url_atual = self.verificar_proxima_pagina(next_page_selector)
            pagina_numero += 1

        return dados_coletados

    def fechar(self):
        """Fecha o navegador."""
        self.driver.quit()


executor = ThreadPoolExecutor(max_workers=3)


# FUN√á√ÉO PARA EXECUTAR SCRAPER EM BACKGROUND
def executar_scraper_background(
    tarefa_id: str,
    config: ConfiguracaoScraper
):
    """Executa o scraper em thread separada"""
    try:
        atualizar_tarefa(
            tarefa_id,
            status="em_progresso",
            mensagem="Iniciando scraper...",
            progresso=5
        )

        print(f"\n{'='*70}")
        print(f"üöÄ EXECUTANDO TAREFA {tarefa_id}")
        print(f"{'='*70}\n")
        scraper = WebScraperComPaginacao(driver_path=config.driver_path)

        try:
            resultados = scraper.processar_todas_paginas(
                url_inicial=config.url_inicial,
                section_selector=config.section_selector,
                li_selector=config.li_selector,
                next_page_selector=config.next_page_selector,
                max_paginas=config.max_paginas
            )

            # Salva em Excel se solicitado
            if config.salvar_excel and resultados:
                salvar_em_excel(
                    resultados,
                    caminho_pasta=settings.DIR_BASE,
                    nome_arquivo=settings.BASE,
                    auto_versionar=False
                )
            else:
                salvar_em_csv(
                    resultados,
                    caminho_pasta=settings.DIR_BASE,
                    nome_arquivo=settings.BASE,
                    auto_versionar=False
                )

            atualizar_tarefa(
                tarefa_id,
                status="concluido",
                progresso=100,
                mensagem=f"‚úì Scraping conclu√≠do! {len(resultados)} produtos coletados.",
                resultados=resultados,
                erro=None,
                timestamp_conclusao=datetime.now().isoformat()
            )

            print(f"\n{'='*70}")
            print(f"‚úÖ TAREFA {tarefa_id} CONCLU√çDA")
            print(f"üì¶ Total de produtos: {len(resultados)}")
            print(f"{'='*70}\n")

        finally:
            scraper.fechar()

    except Exception as e:
        print(f"\n{'='*70}")
        print(f"‚ùå ERRO NA TAREFA {tarefa_id}")
        print(f"Erro: {str(e)}")
        print(f"{'='*70}\n")

        atualizar_tarefa(
            tarefa_id,
            status="erro",
            progresso=0,
            mensagem=f"Erro: {str(e)}",
            resultados=None,
            erro=str(e),
            timestamp_conclusao=datetime.now().isoformat()
        )
